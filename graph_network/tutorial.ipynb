{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df = pd.read_json(\"ratgeber_pages_url.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix build\n",
    "To build the matrix, we need to list every single linked URL in the X-axis. The individual pages that were scraped are listed in the Y-axis.\n",
    "\n",
    "In the first step, we need the URL of each scraped page in the form of a list. These can be found in the \"url\" column.\n",
    "\n",
    "In the next step, we extract each individual URL from the column of URLs linked on the page, if it is not already in the all_linked_page_urls list, it is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scraped_page_urls = df.url.tolist()\n",
    "len(all_scraped_page_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_page_urls_of_linkedpages_column(pandas_dataframe, column_of_lists_with_linked_pages=\"linkedpages\"):\n",
    "    all_linked_page_urls = []\n",
    "    for i in pandas_dataframe.index:\n",
    "        list_of_linked_pages_per_url = pandas_dataframe.loc[i, column_of_lists_with_linked_pages]\n",
    "        for linkedpage in list_of_linked_pages_per_url:\n",
    "            if linkedpage not in all_linked_page_urls:\n",
    "                all_linked_page_urls.append(linkedpage)\n",
    "    return all_linked_page_urls\n",
    "\n",
    "all_linked_page_urls = get_all_page_urls_of_linkedpages_column(df)\n",
    "len(all_linked_page_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_der_verbindungen = []\n",
    "for i in df.index:\n",
    "    list_of_linked_pages_per_url = df.loc[i, \"linkedpages\"]\n",
    "    li = [df.loc[i, \"url\"]] * len(list_of_linked_pages_per_url)\n",
    "    data = list(zip(li, list_of_linked_pages_per_url))\n",
    "    liste_der_verbindungen += data\n",
    "len(liste_der_verbindungen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Matrix\n",
    "\n",
    "Next, we want to fill the matrix. If the respective page (row) in your content links to the other website as an X-axis attribute, enter True, otherwise False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>page_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>page_url_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>page_url_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>page_url_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       A      B      C      D      E      F      G      H    page_url\n",
       "0   True  False  False   True  False  False  False   True  page_url_1\n",
       "1  False  False  False   True  False   True  False  False  page_url_2\n",
       "2  False  False   True  False   True   True  False   True  page_url_3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To figure out how to make it work, I've used this simple Test\n",
    "liste_mit_allen_linked_urls = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "page_url_1 = [\"A\", \"H\", \"D\"]\n",
    "page_url_2 = [\"D\", \"F\"]\n",
    "page_url_3 = [\"C\", \"E\", \"F\", \"H\"]\n",
    "liste_aller_page_urls = [page_url_1, page_url_2, page_url_3]\n",
    "\n",
    "test_matrix = pd.DataFrame(columns=liste_mit_allen_linked_urls)\n",
    "\n",
    "for page_url in liste_aller_page_urls:\n",
    "    ist_in_liste_mit_allen_linked_urls = [(buchstabe in page_url) for buchstabe in liste_mit_allen_linked_urls]\n",
    "    new_row = dict(zip(test_matrix.columns, ist_in_liste_mit_allen_linked_urls))      \n",
    "    test_matrix = pd.concat([test_matrix, pd.DataFrame([new_row])], ignore_index=True)\n",
    "test_matrix = test_matrix.assign(page_url=['page_url_1', 'page_url_2', 'page_url_3'])\n",
    "test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_linked_page_urls = get_all_page_urls_of_linkedpages_column(df)\n",
    "\n",
    "# if all_linked_page_urls[0] != \"page_url\":\n",
    "#     all_linked_page_urls.insert(0, \"page_url\")\n",
    "\n",
    "# assert all_linked_page_urls[0] == \"page_url\", \"first Element must be 'page_url'.\"\n",
    "# assert all_linked_page_urls[1].startswith(\"https://\"), \"second Element must start with 'https://...'.\" \n",
    "\n",
    "\n",
    "# matrix = pd.DataFrame(columns=all_linked_page_urls)\n",
    "# all_scraped_page_urls = df.url.tolist()\n",
    "\n",
    "\n",
    "# for page_url in all_scraped_page_urls:\n",
    "#     idx = all_scraped_page_urls.index(page_url)\n",
    "#     list_of_linked_pages_per_url = df.loc[df.index[df[\"url\"]==page_url].tolist(), \"linkedpages\"]\n",
    "#     is_in_list_of_all_linked_urls = [(linked_page in list_of_linked_pages_per_url[idx]) for linked_page in all_linked_page_urls]\n",
    "#     is_in_list_of_all_linked_urls[0] = page_url\n",
    "#     new_row = dict(zip(matrix.columns, is_in_list_of_all_linked_urls))      \n",
    "#     matrix = pd.concat([matrix, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection_matrix(pandas_dataframe, column_containing_list_of_all_linked_pages_per_url):\n",
    "\n",
    "    all_linked_page_urls = get_all_page_urls_of_linkedpages_column(pandas_dataframe)\n",
    "\n",
    "    if all_linked_page_urls[0] != \"page_url\":\n",
    "        all_linked_page_urls.insert(0, \"page_url\")\n",
    "\n",
    "    assert all_linked_page_urls[0] == \"page_url\", \"first Element must be 'page_url'.\"\n",
    "    assert all_linked_page_urls[1].startswith(\"https://\"), \"second Element must start with 'https://...'.\" \n",
    "\n",
    "    matrix = pd.DataFrame(columns=all_linked_page_urls)\n",
    "    all_scraped_page_urls = pandas_dataframe.url.tolist()\n",
    "\n",
    "    for page_url in all_scraped_page_urls:\n",
    "        idx = all_scraped_page_urls.index(page_url)\n",
    "        list_of_linked_pages_per_url = pandas_dataframe.loc[pandas_dataframe.index[pandas_dataframe[\"url\"]==page_url].tolist(), column_containing_list_of_all_linked_pages_per_url]\n",
    "        is_in_list_of_all_linked_urls = [(linked_page in list_of_linked_pages_per_url[idx]) for linked_page in all_linked_page_urls]\n",
    "        is_in_list_of_all_linked_urls[0] = page_url\n",
    "        new_row = dict(zip(matrix.columns, is_in_list_of_all_linked_urls))      \n",
    "        matrix = pd.concat([matrix, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "df = pd.read_json(\"ratgeber_pages_url.json\")\n",
    "matrix = get_connection_matrix(pandas_dataframe=df, column_containing_list_of_all_linked_pages_per_url=\"linkedpages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "False                                                                               509\n",
       "True                                                                                 59\n",
       "https://www.mobiliar.ch/versicherungen-und-vorsorge/wohnen-und-eigentum/ratgeber      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "matrix.iloc[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_nodes_from(test_matrix['page_url'])\n",
    "\n",
    "for idx, row in test_matrix.iterrows():\n",
    "    start_node = row[\"page_url\"]\n",
    "    for column in test_matrix.columns:\n",
    "        if test_matrix.loc[idx,column] == True:\n",
    "            end_node = column\n",
    "            G.add_edge(start_node, end_node)\n",
    "            \n",
    "print(G.edges)\n",
    "print(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=500, font_size=8)\n",
    "plt.title('Graph der Verbindungen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_nodes_from(matrix['page_url'])\n",
    "\n",
    "for idx, row in matrix.iterrows():\n",
    "    start_node = row[\"page_url\"]\n",
    "    for column in matrix.columns:\n",
    "        if matrix.loc[idx,column] == True:\n",
    "            print(idx,column)\n",
    "            end_node = column\n",
    "            G.add_edge(start_node, end_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=500, font_size=8)\n",
    "plt.title('Graph der Verbindungen')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WBTH-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
