{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import get_graph_from_cluster_data_without_color, get_graph_from_matrix_customized_color, save_as_json\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df = pd.read_json(\"../graph_network/full_data_iv_29-04-24.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix build\n",
    "To build the matrix, we need to list every single linked URL in the X-axis. The individual pages that were scraped are listed in the Y-axis.\n",
    "\n",
    "In the first step, we need the URL of each scraped page in the form of a list. These can be found in the \"url\" column.\n",
    "\n",
    "In the next step, we extract each individual URL from the column of URLs linked on the page, if it is not already in the all_linked_page_urls list, it is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection_matrix(pandas_dataframe, column_containing_list_of_all_linked_pages_per_url):\n",
    "\n",
    "    column_names = pandas_dataframe.url.tolist()\n",
    "\n",
    "    if column_names[0] != \"url\":\n",
    "        column_names.insert(0, \"url\")\n",
    "\n",
    "    assert column_names[0] == \"url\", \"first Element must be 'url'.\"\n",
    "    assert column_names[1].startswith(\"https://\"), \"second Element must start with 'https://...'.\" \n",
    "\n",
    "    matrix = pd.DataFrame(columns=column_names)\n",
    "    list_with_no_linked_pages = []\n",
    "    all_scraped_page_urls = pandas_dataframe.url.tolist()\n",
    "\n",
    "    for url in all_scraped_page_urls:\n",
    "        idx = all_scraped_page_urls.index(url)\n",
    "        list_of_linked_pages_per_url = pandas_dataframe.loc[pandas_dataframe.index[pandas_dataframe[\"url\"]==url].tolist(), column_containing_list_of_all_linked_pages_per_url]\n",
    "        is_in_list_of_all_linked_urls = [(linked_page in list_of_linked_pages_per_url[idx]) for linked_page in column_names]\n",
    "        is_in_list_of_all_linked_urls[0] = url\n",
    "        new_row = dict(zip(matrix.columns, is_in_list_of_all_linked_urls))      \n",
    "        matrix = pd.concat([matrix, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_all_page_urls_of_linkedpages_column(pandas_dataframe, column_of_lists_with_linked_pages=\"linkedpages\"):\n",
    "#     all_linked_page_urls = []\n",
    "#     for i in pandas_dataframe.index:\n",
    "#         list_of_linked_pages_per_url = pandas_dataframe.loc[i, column_of_lists_with_linked_pages]\n",
    "#         for linkedpage in list_of_linked_pages_per_url:\n",
    "#             if linkedpage not in all_linked_page_urls:\n",
    "#                 all_linked_page_urls.append(linkedpage)\n",
    "#     return all_linked_page_urls\n",
    "\n",
    "# all_linked_page_urls = get_all_page_urls_of_linkedpages_column(df)\n",
    "# len(all_linked_page_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scraped_page_urls = df.url.tolist()\n",
    "len(all_scraped_page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(input_vector):\n",
    "    import numpy as np\n",
    "\n",
    "    red = np.array([1, 0, 0])\n",
    "    yellow = np.array([1, 1, 0])\n",
    "    green = np.array([0, 1, 0])\n",
    "\n",
    "    rgb = input_vector[0] * red + input_vector[1] * yellow + input_vector[2] * green\n",
    "\n",
    "    color = \"#\" + \"\".join(f\"{int(x*255):02x}\" for x in rgb)\n",
    "    return color\n",
    "\n",
    "\n",
    "def add_color_column_of_df_to_matrix(matrix, dataframe):\n",
    "    dataframe[\"color\"] = (\n",
    "        dataframe[[\"negative\", \"neutral\", \"positive\"]]\n",
    "        .apply(lambda row: np.array(row), axis=1)\n",
    "        .apply(lambda row: get_color(row))\n",
    "    )\n",
    "    return pd.merge(matrix, dataframe[[\"url\", \"color\"]], how=\"left\", on=[\"url\"])\n",
    "\n",
    "\n",
    "def get_matrix_and_full_attributes_dataset(sentiment_data, scrape_data):\n",
    "    s = pd.merge(\n",
    "        sentiment_data, scrape_data[[\"url\", \"linkedpages\"]], how=\"left\", on=[\"url\"]\n",
    "    )  # .insert(loc=3, column=\"linkedpages\", value=list_of_lipages)\n",
    "    m = get_connection_matrix(scrape_data, \"linkedpages\")\n",
    "    M = add_color_column_of_df_to_matrix(m, sentiment_data)\n",
    "    S = pd.merge(s, M[[\"url\", \"color\"]], how=\"left\", on=[\"url\"])\n",
    "    S = S[\n",
    "        [\n",
    "            \"url\",\n",
    "            \"pagetitle\",\n",
    "            \"negative\",\n",
    "            \"neutral\",\n",
    "            \"positive\",\n",
    "            \"color\",\n",
    "            \"text\",\n",
    "            \"linkedpages\",\n",
    "        ]\n",
    "    ]\n",
    "    return M, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [\"D\", \"H\"]\n",
    "B = [\"A\", \"C\"]\n",
    "C = [\"A\", \"B\", \"C\", \"D\"]\n",
    "D = [\"B\", \"C\", \"H\"]\n",
    "\n",
    "data = {\n",
    "    \"url\": [\"A\", \"B\", \"C\", \"D\"],\n",
    "    \"linkedpages\": [A, B, C, D,]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "all_scraped_page_urls = df.url.tolist()\n",
    "column_names = df.url.tolist()\n",
    "column_names.insert(0, \"url\")\n",
    "\n",
    "matrix = pd.DataFrame(columns=column_names)\n",
    "\n",
    "for page_url in all_scraped_page_urls:\n",
    "    idx = all_scraped_page_urls.index(page_url)\n",
    "    list_of_linked_pages_per_url = df.loc[df.index[df[\"url\"]==page_url].tolist(), \"linkedpages\"]\n",
    "    is_in_list_of_all_linked_urls = [(linked_page in list_of_linked_pages_per_url[idx]) for linked_page in column_names]\n",
    "    is_in_list_of_all_linked_urls[0] = page_url\n",
    "    new_row = dict(zip(matrix.columns, is_in_list_of_all_linked_urls))      \n",
    "    matrix = pd.concat([matrix, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# carac = pd.DataFrame({ 'page_url':['A', 'B', 'C', 'D'], 'color':[(0,0.5,0),(0.86,0,0),(1,1,0),(0.8,0.25,0)]})\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"url\": [\"A\", \"B\", \"C\", \"D\"],\n",
    "    \"negative\": [0.4, 0.8, 0.0, 0.1],\n",
    "    \"neutral\": [0.2, 0.2, 0.1, 0.8],\n",
    "    \"positive\": [0.4, 0.0, 0.9, 0.1],\n",
    "}\n",
    "sentiment_data = pd.DataFrame(data)\n",
    "sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = add_color_column_of_df_to_matrix(matrix, sentiment_data)\n",
    "test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run code on Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = pd.read_json(\"../sentiment-analyser/saiv_28-04-24.json\")\n",
    "scrape_data = pd.read_json(\"../web-crawler/scrapy_mobiliar/mobiscraper/spiders/scrape_archive/full_scrape_IV.json\")\n",
    "matrix_with_color_column, full_data = get_matrix_and_full_attributes_dataset(sentiment_data, scrape_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_from_matrix_customized_color(\n",
    "    matrix_df, color_col=\"color\", edge_color=\"#87edec\"\n",
    "):\n",
    "    import networkx as nx\n",
    "    from pyvis.network import Network\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for idx, row in matrix_df.iterrows():\n",
    "        G.add_node(row[\"url\"], color=row[color_col], size=120)\n",
    "        start_node = row[\"url\"]\n",
    "        for column in matrix_df.columns:\n",
    "            if matrix_df.loc[idx, column] == True:\n",
    "                end_node = column\n",
    "                G.add_edge(start_node, end_node, color=edge_color)\n",
    "\n",
    "    N = Network(\n",
    "        height=\"1500px\",\n",
    "        width=\"100%\",\n",
    "        bgcolor=\"#222222\",\n",
    "        font_color=\"white\",\n",
    "        directed=True,\n",
    "        notebook=False,\n",
    "    )\n",
    "    N.barnes_hut(\n",
    "        gravity=-80000,\n",
    "        central_gravity=0.3,\n",
    "        spring_length=250,\n",
    "        spring_strength=0.001,\n",
    "        damping=0.09,\n",
    "        overlap=0,\n",
    "    )\n",
    "    N.from_nx(G)\n",
    "\n",
    "    N.show_buttons()\n",
    "\n",
    "    for node in N.nodes:\n",
    "        node_id = node[\"id\"]\n",
    "        node[\"color\"] = G.nodes[node_id].get(\"color\", \"gray\")\n",
    "    return G, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, N = get_graph_from_matrix_customized_color(matrix_with_color_column, color_col=\"color\", edge_color=\"#018786\")\n",
    "N.show(\"graph_sentiment_coloring_iv.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WBTH-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
